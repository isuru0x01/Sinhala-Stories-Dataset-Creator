# -*- coding: utf-8 -*-
"""Sinhala Stories Dataset HuggingFace.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xx3e1qo9Ie0WdJ7lInCX0raOaQdLaXTA
"""

from datasets import load_dataset

dataset = load_dataset("Isuru0x01/sinhala_stories")
print(dataset['train'][:3])  # View first 3 rows

# View first few rows
for i, story in enumerate(dataset['train']['story'][:3]):
    print(f"\n--- Story {i+1} ---")
    print(story[:200])  # Print first 200 characters

print("\n" + "="*50 + "\n")

print(dataset)

dataset2 = load_dataset("polyglots/MADLAD_CulturaX_cleaned")

print(dataset2)

# View first few rows
for i, story in enumerate(dataset2['train']['text'][:3]):
    print(f"\n--- Story {i+1} ---")
    print(story[:])

print("\n" + "="*50 + "\n")

from datasets import load_dataset, Dataset, DatasetDict, concatenate_datasets

# Rename 'text' to 'story' and concatenate
# This creates a new dataset with only the 'story' column
dataset2_renamed = dataset2['train'].rename_column('text', 'story')
dataset2_story_only = dataset2_renamed.remove_columns(['lang', 'src'])

# Concatenate the datasets
combined_dataset = concatenate_datasets([dataset['train'], dataset2_story_only])

print(f"Combined dataset rows: {len(combined_dataset)}")
print(f"Combined dataset features: {combined_dataset.features}")
print(f"\nFirst row from dataset1: {combined_dataset[0]['story'][:100]}...")
print(f"\nLast row from dataset2: {combined_dataset[-1]['story'][:100]}...")

# Save the combined dataset
combined_dataset_dict = DatasetDict({'train': combined_dataset})
print(f"\nFinal structure:")
print(combined_dataset_dict)

# Save the combined dataset
combined_dataset_dict = DatasetDict({'train': combined_dataset})
print(f"\nFinal structure:")
print(combined_dataset_dict)

# Save to disk (backup)
combined_dataset_dict.save_to_disk("./combined_sinhala_dataset")

# Login to Hugging Face (you'll need your token)
from huggingface_hub import login

# Method 1: Login interactively (recommended for first time)
# login()  # This will prompt you to enter your token

# Method 2: Login with token directly
login(token="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")

# Push to Hugging Face Hub (this will update the existing dataset)
combined_dataset_dict.push_to_hub(
    "Isuru0x01/sinhala_stories",
    private=False  # Set to True if you want it private
)

print("\nâœ… Dataset successfully uploaded to Hugging Face!")
print("URL: https://huggingface.co/datasets/Isuru0x01/sinhala_stories")

print("\n" + "="*50 + "\n")

# ===== STEP 1: Install and setup =====
!pip install -q datasets huggingface_hub tqdm

import os
import gc
os.environ['HF_DATASETS_CACHE'] = '/content/hf_cache'
gc.enable()

from datasets import load_dataset, Dataset, DatasetDict
from huggingface_hub import login
import hashlib
from tqdm.auto import tqdm

# ===== STEP 3: Login to Hugging Face =====
# You'll be prompted to enter your token
#login()

# Alternative: Provide token directly
login(token="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")

# ===== STEP 3: Configuration =====
DATASET_REPO = "Isuru0x01/sinhala_stories"
COMMIT_HASH = "c5cf62d48169879b701ba18b79779545edefa93e"
CHUNK_SIZE = 5000  # Process in smaller chunks

print(f"ğŸ“¦ Dataset: {DATASET_REPO}")
print(f"ğŸ”– Target commit: {COMMIT_HASH}")
print("="*60)

# ===== STEP 4: Load datasets with streaming (ultra low memory) =====
print("\nğŸ“¥ Loading datasets in streaming mode...")

# Option 1: Use streaming mode (doesn't load entire dataset into RAM)
old_dataset_stream = load_dataset(
    DATASET_REPO,
    split="train",
    revision=COMMIT_HASH,
    streaming=True  # This loads data on-the-fly
)

current_dataset_stream = load_dataset(
    DATASET_REPO,
    split="train",
    streaming=True
)

print("âœ… Datasets loaded in streaming mode (minimal RAM usage)")

# ===== STEP 5: Build hash set from current dataset (in chunks) =====
print("\nğŸ”„ Building hash index from current dataset...")
current_hashes = set()

for i, example in enumerate(tqdm(current_dataset_stream, desc="Indexing current")):
    story_hash = hashlib.md5(example['story'][:200].encode('utf-8')).hexdigest()
    current_hashes.add(story_hash)

    # Periodic garbage collection
    if i % 50000 == 0 and i > 0:
        gc.collect()

print(f"âœ… Indexed {len(current_hashes):,} stories from current dataset")

# ===== STEP 6: Filter and save unique stories to disk (chunk by chunk) =====
print("\nğŸ“ Finding unique stories from old dataset...")

import tempfile
import json

# Create temporary file to store unique stories
temp_file = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.jsonl')
unique_count = 0
duplicate_count = 0

print(f"ğŸ’¾ Saving unique stories to: {temp_file.name}")

for i, example in enumerate(tqdm(old_dataset_stream, desc="Processing old dataset")):
    story = example['story']
    story_hash = hashlib.md5(story[:200].encode('utf-8')).hexdigest()

    if story_hash not in current_hashes:
        # Write to temporary file (streaming write - no RAM buildup)
        json.dump({"story": story}, temp_file)
        temp_file.write('\n')
        unique_count += 1
    else:
        duplicate_count += 1

    # Periodic garbage collection and progress update
    if i % 10000 == 0 and i > 0:
        gc.collect()
        temp_file.flush()

temp_file.close()

print(f"\nâœ… Processing complete:")
print(f"   - Unique stories to recover: {unique_count:,}")
print(f"   - Duplicates skipped: {duplicate_count:,}")

# Clear memory
del current_hashes
gc.collect()

# ===== STEP 7: Load recovered stories from temp file =====
if unique_count > 0:
    print("\nğŸ“¥ Loading recovered stories from temporary file...")
    recovered_dataset = load_dataset('json', data_files=temp_file.name, split='train')
    print(f"âœ… Loaded {len(recovered_dataset):,} recovered stories")

    # Show preview
    print(f"\n   First recovered story (preview):")
    print(f"   {recovered_dataset['story'][0][:150]}...")
else:
    print("\nâš ï¸ No unique stories to recover")
    recovered_dataset = None

# ===== STEP 8: Load current dataset normally for merging =====
if recovered_dataset is not None and len(recovered_dataset) > 0:
    print("\nğŸ“¥ Loading current dataset for merge...")
    current_dataset = load_dataset(DATASET_REPO, split="train")
    print(f"âœ… Current dataset: {len(current_dataset):,} rows")

    # ===== STEP 9: Merge datasets =====
    print("\nğŸ”— Merging datasets...")
    from datasets import concatenate_datasets

    merged_dataset = concatenate_datasets([current_dataset, recovered_dataset])
    print(f"âœ… Merged dataset: {len(merged_dataset):,} rows")

    # Clear memory
    del current_dataset
    del recovered_dataset
    gc.collect()

# ===== STEP 10: Save backup =====
print("\nğŸ’¾ Saving backup locally...")
merged_dataset.save_to_disk("./merged_dataset_backup")
print("âœ… Backup saved")

# ===== STEP 11: Preview =====
print("\n" + "="*60)
print("ğŸ“Š FINAL DATASET PREVIEW")
print("="*60)
print(f"Total stories: {len(merged_dataset):,}")
print(f"\nFirst story: {merged_dataset['story'][0][:100]}...")
print(f"\nLast story: {merged_dataset['story'][-1][:100]}...")

# ===== STEP 12: Upload confirmation =====
print("\n" + "="*60)
print("âš ï¸ READY TO UPLOAD")
print("="*60)

user_input = input("\nâš ï¸ Upload to Hugging Face? (yes/no): ")

if user_input.lower() in ['yes', 'y']:
    print("\nâ˜ï¸ Uploading...")
    merged_dataset_dict = DatasetDict({'train': merged_dataset})
    merged_dataset_dict.push_to_hub(
        DATASET_REPO,
        commit_message=f"Recover {unique_count:,} stories from commit {COMMIT_HASH[:7]}"
    )
    print("âœ… SUCCESS! Dataset uploaded")
    print(f"ğŸ”— https://huggingface.co/datasets/{DATASET_REPO}")
else:
    print("\nâŒ Upload cancelled")

# ===== Cleanup =====
print("\nğŸ§¹ Cleaning up temporary files...")
try:
    os.unlink(temp_file.name)
    print("âœ… Temporary files removed")
except:
    pass

print("\n" + "="*60)
print("âœ… PROCESS COMPLETE")
print("="*60)